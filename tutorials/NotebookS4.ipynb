{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook S4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This requires the following installation:\n",
    "pip install azure==4.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.storage.blob as azureblob\n",
    "import azure.batch.batch_service_client as batch\n",
    "import azure.batch.batch_auth as batchauth\n",
    "import azure.batch.models as batchmodels\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect Client to the blob storage account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the account key by going to >> Access Keys under the Storage Account.\n",
    "The storage account name and access key are both given on this page. Also, see TutorialS2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the account:\n",
    "_STORAGE_ACCOUNT_NAME = 'seismicloud2'\n",
    "# Key to the account\n",
    "# KEEP THIS SECURE\n",
    "# Should be a long string of letters, numbers, and symbols\n",
    "_STORAGE_ACCOUNT_KEY = 'your_key_here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a tie to the storage account using the azure python libraries\n",
    "blob_client = azureblob.BlockBlobService(\n",
    "        account_name=_STORAGE_ACCOUNT_NAME,\n",
    "        account_key=_STORAGE_ACCOUNT_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get SAS URL token to access the container within the blob storage account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_container_sas_token(block_blob_client,\n",
    "                            container_name, blob_permissions):\n",
    "    \"\"\"\n",
    "    Obtains a shared access signature granting the specified permissions to the\n",
    "    container.\n",
    "    :param block_blob_client: A blob service client.\n",
    "    :type block_blob_client: `azure.storage.blob.BlockBlobService`\n",
    "    :param str container_name: The name of the Azure Blob storage container.\n",
    "    :param BlobPermissions blob_permissions:\n",
    "    :rtype: str\n",
    "    :return: A SAS token granting the specified permissions to the container.\n",
    "    \"\"\"\n",
    "    # Obtain the SAS token for the container, setting the expiry time and\n",
    "    # permissions. In this case, no start time is specified, so the shared\n",
    "    # access signature becomes valid immediately. Expiration is in 2 hours.\n",
    "    container_sas_token = \\\n",
    "        block_blob_client.generate_container_shared_access_signature(\n",
    "            container_name,\n",
    "            permission=blob_permissions,\n",
    "            expiry=datetime.datetime.utcnow() + datetime.timedelta(hours=2))\n",
    "\n",
    "    return container_sas_token\n",
    "\n",
    "def get_container_sas_url(block_blob_client,\n",
    "                          container_name, blob_permissions):\n",
    "    \"\"\"\n",
    "    Obtains a shared access signature URL that provides write access to the \n",
    "    ouput container to which the tasks will upload their output.\n",
    "    :param block_blob_client: A blob service client.\n",
    "    :type block_blob_client: `azure.storage.blob.BlockBlobService`\n",
    "    :param str container_name: The name of the Azure Blob storage container.\n",
    "    :param BlobPermissions blob_permissions:\n",
    "    :rtype: str\n",
    "    :return: A SAS URL granting the specified permissions to the container.\n",
    "    \"\"\"\n",
    "    # Obtain the SAS token for the container.\n",
    "    sas_token = get_container_sas_token(block_blob_client,\n",
    "                                        container_name, azureblob.BlobPermissions.WRITE)\n",
    "\n",
    "    # Construct SAS URL for the container\n",
    "    container_sas_url = \"https://{}.blob.core.windows.net/{}?{}\".format(\n",
    "        _STORAGE_ACCOUNT_NAME, container_name, sas_token)\n",
    "\n",
    "    return container_sas_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_container_name = 'seismicloud2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_container_sas_url= get_container_sas_url(\n",
    "        blob_client,\n",
    "        output_container_name,\n",
    "        azureblob.BlobPermissions.WRITE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Great! Now we have a secure, direct tie to the storage account. Next we need to tie to the Batch account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect Client to batch account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See TutorialS3 to locate your batch account name and key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_BATCH_ACCOUNT_NAME = 'tmdetect'\n",
    "_BATCH_ACCOUNT_KEY = 'your_key_here'\n",
    "_BATCH_ACCOUNT_URL = 'https://tmdetect.westus2.batch.azure.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Batch service client\n",
    "credentials = batchauth.SharedKeyCredentials(_BATCH_ACCOUNT_NAME,\n",
    "                                             _BATCH_ACCOUNT_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_client = batch.BatchServiceClient(credentials,base_url=_BATCH_ACCOUNT_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we've connected to the batch account, we want to identify the specific Pool you've set up to run on. We can do this simply using its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_POOL_ID = 'zoestest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay, now we can create a job for this pool! A \"job\" is simple to create, because it is basically just an organizational resource- we will then assign \"tasks\" to the \"job\", which is where the real work happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by creating a name for your job\n",
    "_JOB_ID = 'zoestest2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_job(batch_service_client, job_id, pool_id):\n",
    "    \"\"\"\n",
    "    Creates a job with the specified ID, associated with the specified pool.\n",
    "    :param batch_service_client: A Batch service client.\n",
    "    :type batch_service_client: `azure.batch.BatchServiceClient`\n",
    "    :param str job_id: The ID for the job.\n",
    "    :param str pool_id: The ID for the pool.\n",
    "    \"\"\"\n",
    "    print('Creating job [{}]...'.format(job_id))\n",
    "\n",
    "    job = batch.models.JobAddParameter(\n",
    "        id=job_id,\n",
    "        pool_info=batch.models.PoolInformation(pool_id=pool_id))\n",
    "\n",
    "    batch_service_client.job.add(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the job that will run the tasks. Note that this is also connected to the pool!\n",
    "create_job(batch_client, _JOB_ID, _POOL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tasks within job\n",
    "When you create a task within a Job, which is tied to a specific Pool, the job will assign the task to whichever node in the Pool is available. \n",
    "In our workflow, we create the same number of tasks as nodes in the pool. This way, there is no \"queuing\" of tasks and nodes do not communicate to each other.\n",
    "\n",
    "This does mean that there will likely be one node that finishes its task before the others and will be sitting idle while the others are still running.\n",
    "\n",
    "We indicate which node number each task is running on with the command we send to the task.\n",
    "Each task creates the same job list, but then only executes the jobs that have its rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the Docker image you want the task to run within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = 'ghcr.io/denolle-lab/seismicloud:latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify which path to run the Docker image from. \n",
    "Importantly, this needs to match with the pathnames in your config files, so that the running scripts can access both the python scripts and the directory where the storage container is mounted to the node (remember-- we did that in the start-up task when we created the Pool.)\n",
    "\n",
    "We also specify that the commands in the task will run with root permission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_run_options='-u root -v /tmp/data:/tmp/data/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can specify what commands each task will run.\n",
    "This will be a series of two bash commands, similar to how we run locally, with slightly different pathnames and input arguments. \n",
    "This is the same set of commands as shown in Figure 1. First, create the job list, and then use MPI to run the detection script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First- the command to create the job list\n",
    "# Note the variables in curly brackets- these will be filled in later \n",
    "command1 = \"python /tmp/batch_scripts/template_matching/create_joblist.py -c {config_path} -b {n_nodes}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next- the command that runs the detection. Here this is shown for template matching,\n",
    "# but this could be replaced by the script to run EQT detection\n",
    "# Note here that the seismic network we want to run on is specified by -n,\n",
    "# And the year we want to run on by -y\n",
    "# Make sure these are correct for your purposes!\n",
    "# Again, the variables in curly brackets will be filled in later\n",
    "command2 = \"mpirun -np {n_cpus} python /tmp/batch_scripts/template_matching/distributed_detection.py -c {config_path} -n NV -y 2017 -b {idx}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we combine those two commands into one, adding a /bin/bash specification at the beginning\n",
    "# and a double && so that they run sequentially\n",
    "input_command = '/bin/bash -c ' + command1 + ' && ' + command2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tasks(batch_service_client, job_id, image_name,container_run_options,input_command, n_nodes, n_cpus, config_path, output_container_sas_url):\n",
    "    \"\"\"\n",
    "    Adds a task for each input file in the collection to the specified job.\n",
    "    :param batch_service_client: A Batch service client.\n",
    "    :type batch_service_client: `azure.batch.BatchServiceClient`\n",
    "    :param str job_id: The ID of the job to which to add the tasks.\n",
    "    :param list input_files: A collection of input files. One task will be\n",
    "     created for each input file.\n",
    "    :param output_container_sas_token: A SAS token granting write access to\n",
    "    the specified Azure Blob storage container.\n",
    "    \"\"\"\n",
    "\n",
    "    print('Adding {} tasks to job [{}]...'.format(n_nodes, job_id))\n",
    "\n",
    "    tasks = list()\n",
    "\n",
    "    for idx in range(n_nodes):\n",
    "        command = eval('f\"'+input_command+'\"')\n",
    "        task = batch.models.TaskAddParameter(\n",
    "            id='Task{}'.format(idx),\n",
    "            command_line=command,\n",
    "            user_identity=batchmodels.UserIdentity(\n",
    "                auto_user=batchmodels.AutoUserSpecification(\n",
    "                    scope=batchmodels.AutoUserScope.pool,\n",
    "                    elevation_level=batchmodels.ElevationLevel.admin)),\n",
    "            container_settings=batchmodels.TaskContainerSettings(\n",
    "                container_run_options=container_run_options,\n",
    "                image_name = image_name)\n",
    "        )\n",
    "        tasks.append(task)\n",
    "\n",
    "    batch_service_client.task.add_collection(job_id, tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the function defined above, we are all set to add N tasks to our Pool of N Nodes!\n",
    "\n",
    "# Define number of nodes\n",
    "n_nodes = 1\n",
    "# If you need to check how many nodes are currently in the pool, use the following:\n",
    "#n_nodes = batch_client.pool.get(pool_id=_POOL_ID).target_dedicated_nodes\n",
    "\n",
    "# Define number of CPUs per node. This is based on the instance type you chose during Pool construction\n",
    "n_cpus = 4\n",
    "\n",
    "# Define config file path- remember, must work with the specification you are running your docker image from\n",
    "config_path = '/tmp/configs/config_tm_batch.json'\n",
    "\n",
    "# Go ahead and send your tasks!\n",
    "add_tasks(batch_client, _JOB_ID, \n",
    "          image_name,container_run_options,\n",
    "          input_command,\n",
    "          n_nodes, n_cpus, config_path, \n",
    "          output_container_sas_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If that worked successfully, you should be able to check on your nodes in the Pool in the Azure portal. If running, they will appear as green boxes. See Tutorial S3 for a how-to to check on these nodes through the Portal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also check on the nodes from here. Here's some examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether a task is done running on the nodes\n",
    "# The loop below checks on all of them\n",
    "status = []\n",
    "for i in range(n_nodes):\n",
    "    task_id = 'Task{}'.format(i)\n",
    "    node_status = batch_client.task.get(job_id =_JOB_ID,task_id=task_id).state.name\n",
    "    status.append(node_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how long the tasks took to run, if they are finished\n",
    "run_times = []\n",
    "for i in range(n_nodes):\n",
    "    task_id = 'Task{}'.format(i)\n",
    "    start_time = batch_client.task.get(job_id =_JOB_ID,task_id=task_id).creation_time\n",
    "    end_time = batch_client.task.get(job_id =_JOB_ID,task_id=task_id).state_transition_time\n",
    "    run_times.append(end_time-start_time)\n",
    "run_time = max(run_times)\n",
    "print('Run time = ' + str(run_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Also, if you'd like to resize your Pool (add more nodes and run more tasks, OR resize it to zero when you are done running tasks), you can do that from here as well.\n",
    "\n",
    "#### That is why it is so useful to have a Pool set up-- once it is running, you can keep adding or lowering the current compute resources to suit your needs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size up pool, redefine n_nodes\n",
    "n_nodes = n_nodes * 2\n",
    "batch_client.pool.resize(pool_id = _POOL_ID,\n",
    "                 pool_resize_parameter=batch.models.PoolResizeParameter(\n",
    "                     target_dedicated_nodes=n_nodes))\n",
    "print('Sizing up to '+str(n_nodes)+' nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After sending the command to size up, it will take several minutes for the \n",
    "# new nodes in the Pool to become usable.\n",
    "# In this case, you'll want to wait to create a new job until the Pool finishes resizing\n",
    "# Below shows an example of how to check on that:\n",
    "state_ready = False\n",
    "while not state_ready:\n",
    "    time.sleep(20)\n",
    "    print('Checking node state...')\n",
    "    counter = 0\n",
    "    for i in batch_client.compute_node.list(pool_id = _POOL_ID):\n",
    "        # Nodes in the Pool are ready to be used if they are \"idle\"\n",
    "        # This means they have successfully completed their start-up task\n",
    "        if i.state == azure.batch.models.ComputeNodeState.idle:\n",
    "            counter = counter + 1\n",
    "    if counter == n_nodes:\n",
    "        state_ready = True   \n",
    "    else:\n",
    "        state_ready = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once your tasks have finished, you can check on the outputs in the Blob storage container and download the resulting output files using azcopy (see TutorialS2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alaska-ml",
   "language": "python",
   "name": "alaska-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
